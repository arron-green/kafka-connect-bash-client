# Find a full set of options here:
# https://github.com/confluentinc/kafka-connect-storage-cloud/blob/v5.0.0/kafka-connect-s3/docs/configuration_options.rst

name=connect-s3

# Name or alias of the class for this connector.
# Must be a subclass of org.apache.kafka.connect.connector.Connector.
connector.class=io.confluent.connect.s3.S3SinkConnector

auto.offset.reset=earliest

# Maximum number of tasks to use for this connector
tasks.max=1

# a list of topics to process with this connector
# topics=topic1,topic2,topic3

# a regex of topics to process with this connector
topics.regex=^topic-prefix.ExampleEvent$

# Top level directory to store the data ingested from Kafka
topics.dir=topics

# overrides the url to S3 (only used for local environments)
store.url=http://localhost:9000

# The AWS region to be used the connector
s3.region=us-east-1

# The S3 Bucket where data will be written
s3.bucket.name=mys3bucket

# Number of upload retries of a single S3 part. Zero means no retries
s3.part.retries=3

# The Part Size in S3 Multi-part Uploads
s3.part.size=26214400

# flush every 5 minutes, if flush.size has not been met
# according to https://github.com/confluentinc/kafka-connect-storage-cloud/issues/144#issuecomment-377080726
# this breaks exactly once semantics!
# The time interval in milliseconds to periodically invoke file commits.
# This configuration ensures that file commits are invoked every configured interval.
# Time of commit will be adjusted to 00:00 of selected timezone.
# Commit will be performed at scheduled time regardless previous commit time or number of messages.
# This configuration is useful when you have to commit your data based on current server time,
# like at the beginning of every hour.
rotate.schedule.interval.ms=300000

# Number of records written to store before invoking file commits
flush.size=1000

# The time interval in milliseconds to invoke file commits.
# This configuration ensures that file commits are invoked every configured interval.
# This configuration is useful when data ingestion rate is low and the connector didn't write enough messages to commit files
# rotate.interval.ms=1800000
rotate.interval.ms=300000

# The underlying storage layer
storage.class=io.confluent.connect.s3.storage.S3Storage

# The format class to use when writing data to the store
format.class=io.confluent.connect.s3.format.avro.AvroFormat

# The Avro compression codec to be used for output files.
#   Options are: null, deflate, snappy and bzip2
avro.codec=snappy

# The partitioner to use when writing data to the store
# options are:
#   io.confluent.connect.storage.partitioner.DefaultPartitioner
#      preserves the Kafka partitions
#   io.confluent.connect.storage.partitioner.FieldPartitioner
#      partitions the data to different directories according to the value
#      of the partitioning field specified in "partition.field.name"
#   io.confluent.connect.storage.partitioner.TimeBasedPartitioner
#      partitions data according to ingestion time
partitioner.class=io.confluent.connect.storage.partitioner.TimeBasedPartitioner
# The extractor that gets the timestamp for records when partitioning with TimeBasedPartitioner
# options are: Wallclock, Record or RecordField
timestamp.extractor=RecordField
# The record field to be used as timestamp by the timestamp extractor
timestamp.field=_ec_received_time
# The locale to use when partitioning with TimeBasedPartitioner
locale=en_US
# The timezone to use when partitioning with TimeBasedPartitioner
timezone=UTC
# This configuration is used to set the format of the data directories when partitioning with TimeBasedPartitioner.
# The format set in this configuration converts the Unix timestamp to proper directories strings.
# For example, if you set path.format='year'=YYYY/'month'=MM/'day'=dd/'hour'=HH, the data directories
# will have the format /year=2015/month=12/day=07/hour=15/
path.format='year'=YYYY/'month'=MM/'day'=dd
# The duration of a partition milliseconds used by TimeBasedPartitioner
partition.duration.ms=3600000

schema.compatibility=FULL

# Allow connect converter to add its meta data to the output schema
# defaults to true but adds metadata to the schema, disable this
connect.meta.data=false

